## What is the European Open-Source AI index?
The European Open-Source AI Index collects information on model openness, licensing, and EU regulation of generative AI systems and providers.

The index is a non-profit public resource hosted at the [Centre of Language and Speech Technology, Radboud University, The Netherlands](https://www.ru.nl/en/cls/clst), maintained by a small team of academics and community members.

#### Founders: 

<img style="float: left;" src="/images/al.jpg" alt="Andreas Liesenfeld"  style="height: 100px; width:100px;" >

__Andreas Liesenfeld__ is co-founder of the index and Assistant Professor at Radboud University where he researches the societal and regulatory implications of emerging language technologies. His current work focuses on technology assessment, open source generative AI, and comparative AI regulation. At the Centre for Language Studies (CLS), Andreas Liesenfeld is a member of the language technology expertise group and, in collaboration with the Netherlands eScience center, develops open source research software for natural language processing.

<img style="float: left;" src="/images/md.png" alt="Mark Dingemanse"  style="height: 100px; width:100px;" >

__Mark Dingemanse__ is co-founder of the EU OSAI index and Associate Professor at Radboud University. He carries out observational studies, experiments and simulations in order to find out why languages are the way they are. In 2020 he was awarded the Heineken Young Scientists Award in Humanities, and he directs a research group _[Futures of Language](https://markdingemanse.net/futures)_ funded by a VICI talent grant from the Dutch Research Council (2024-2029). His current research looks at the intersection of language and technology, from the artisanal tools that people use in everyday conversation to the artificial techniques of large language models, voice user interfaces, and "AI".

#### Contributors: 

__Dick Blankvoort__

__Adem Kaya__

## Why is open-source AI important?
Open-source software and systems are a crucial building block of innovative technology ecosystems and provide an important alternative to proprietary, closed systems for users and builders of generative AI systems. True openness fosters innovation that not only serves commercial interests, but also contributes to knowledge access, free education, and a more informed public debate about this emerging technology.

[Read more](/about)

## How is open-source AI regulated in the EU?
In 2025, the AI landscape will be shaken up by the enforcement of the EU AI Act, the world's first comprehensive AI law, with a projected impact comparable to GDPR. This regulation pays special attention to open-source systems and regulates ‘open’ systems differently. But the EU AI Act neither defines openness nor specifies how open-source licenses relate to openness. This work will be carried out by the future EU AI Office.

[Read more](/about)

## What are the risks when the definition of ‘open’ is still open?
The unclear definition of openness in the domain of generative AI creates loopholes for overclaiming openness (by ie. using an open source license for a single aspect) and the risk of unintended claims to exemptions based on open-source status under EU law. A practice known as ‘open-washing’.

[Read more](/about)

## How we evaluate openness
We created an evidence-based framework that distinguishes 14 dimensions of openness, from training datasets to scientific and technical documentation and from licensing to access methods. We published our research in a paper. The data is continuously updated and publicly available via our [GitHub repository](https://github.com/Language-Technology-Assessment/main-database). **Contributions are encouraged**. The website you’re currently reading is providing a user-friendly interface for the data from the Github repository and gives extra contextual information to a wider audience.

[Read more](/about)

## Why did we do this?
Evidence-based openness assessment can help foster a generative AI landscape in which models can be effectively regulated, model providers can be held accountable, scientists can scrutinise generative AI, and end users can make informed decisions.
