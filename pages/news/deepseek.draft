---
title: How Open-Source is DeepSeek-R1?
description: Investigating the open-source status of DeepSeek-R1
date: 29-01-2025
author: Dick Blankvoort
---
# How Open-Source is DeepSeek-R1?
<author :author="author"></author>

In the past few days, a great amount of attention has been heaped on DeepSeek-R1. This open model claims superior performance to GPT-4o while having been trained at a fraction of the cost and with great efficiency. In this blog-post, we seek to make clear how DeepSeek was developed and how it positions itself in the open-source landscape more broadly.


## Development of DeepSeek
The development of DeepSeek can be visually depicted as follows:
![Diagram depicting the development of DeepSeek](/images/deepseek_dev.png "DeepSeek dev map")

The first generation of the DeepSeek models (DeepSeek-LLM-Base and DeepSeek-LLM-Chat) for the most part used a relatively straightforward recipe. A 'base' LLM was trained using a large amount of data (in this case data gathered by the CommonCrawl, a project which archives most of the public-facing internet), and the model was subsequently fine-tuned to work well as a chat model using a combination of supervised fine-tuning (SFT) and direct preference optimization (DPO). The key innovation which DeepSeek-LLM brought to the table was that it filtered its data very well, keeping only the most essential and highest-quality data to learn from. The largest of the first generation base models claimed superior performance to Llama-2-70B across a range of benchmarks, and the chat model even claimed superior performance to GPT-3.5.

DeepSeek-V2 proceeded to depart from this relatively straightforward recipe by adopting a mixture-of-experts (MoE) approach. In this approach, many models are trained in parallel to each specialize in a specific knowledge domain ('expert models'). When generating text the overarching 'parent' model then switches between which expert it uses to generate text depending on the context. The creators of DeepSeek trained a single mixture-of-experts 'base' model using curated data, and subsequently experimented with what data would work best for fine-tuning it by constructing several generations of chat models. A key discovery during this latter stage was that integrating data used for code-tuning a model (optimizing it to generate code well) could improve performance in both chat and code settings. Models which integrated both code and regular instruction data were referred to as DeepSeek-V2.5.

DeepSeek-V3 fundamentally built on the same approach as DeepSeek-V2 while using more and better data. The model was scaled up to three times the size of DeepSeek-V2, and for training an increased amount of mathematics, programming, and multilingual data was incorporated. The model also integrated some novel techniques to better balance the different experts between each other. This third-generation model is the model which was used to construct DeepSeek-R1.

For the second generation, the model creators took a base model which they had taken a great amount of effort to train and explored various ways to best tune it to function as a chat-bot. For the third generation, largely the same principle seems to have been followed. The model creators first attempted to use a novel reinforcement learning technique (GRPO) to endow the base model with a greater understanding of reasoning. This technique was already used with success in the mathematically-tuned cousins of DeepSeek in order to provide mathematical reasoning, and so the step towards using it to endow a model with general reasoning capabilities appears logical.

The model which resulted from this was DeepSeek-R1-Zero. Unfortunately, although the model seemed to have improved reasoning capabilities, the technique used also resulted in quite a few instabilities in how the model behaved. The model was prone to switching between languages, not adhering to formats, and in general providing poorly-readable responses. Nonetheless, the techniques experimented with here evidently proved valuable for subsequent development.

The next publicly-released fine-tuning attempt was DeepSeek-V3. This model experimented with using synthetic data (data generated by a different LLM) to better learn how to serve as a chat-bot. It used filtered output from both DeepSeek-R1-Zero and DeepSeek-V2.5 to learn how to perform both reasoning and non-reasoning tasks well. Fine-tuning is commonly said to require far less compute than pre-training, and the same held here. The model was fully fine-tuned using only 1.5 million `instances' of instructions to learn from. Though the performance of this model was quite promising, subsequent development continued.

The third fine-tuning attempt was the one which ended up making news headlines. The key idea with this attempt was to take the approach used to train DeepSeek-R1-Zero while prepending a small phase of `traditional' instruct-tuning. It seems the model creators theorized that doing so could resolve the observed instabilities. This turned out to indeed be the case, as the performance of the tuned model rivals GPT-4o.

## Open-source
One major question which is especially pertinent to the index is that of how open-source DeepSeek-R1 is. This, to a large extent, also determines how much the open-source community can take advantage of the model's capabilities. After performing an investigation, we obtained the following results on our key evaluation points:

### Base model / end model data
For both its training and chat-tuning, DeepSeek makes use of a proprietary dataset. Though the paper of the original DeepSeek-LLM suggests that this data is primarily obtained from the CommonCrawl, a lack of transparancy of how and what data is gathered means that the exact data mixture is unknown. Methods for deduplicating and curating the data are described in earlier papers, however in too abstract terms to enable accurate reproduction.

It could be that this is a deliberate strategy on the part of DeepSeek, as it seems that their primary strategic advantage lies in their highly curated and high-quality dataset. By not publishing it, they are free to publish the techniques involved in training their models without having to worry about a superior-performance competitor emerging in the near future. In any case, further insight is warranted into the datasets used in training DeepSeek.

### Base / end model weights
DeepSeek publishes both the base and the end model weights of all of their chat-tuned models. This allows for researchers to further build upon their models in order to construct their own LLMs and in general greatly improves the utility of their models. However, as discussed in previous posts, merely publishing the weights of a model is not enough for it to be considered truly open-source.

### Training code
Here is where we start running into issues regarding the novelty of DeepSeek-R1. Though the repository of the base model DeepSeek-V3-Base does contain very limited code for building on the model, such code has not yet been added to DeepSeek-R1. In general, the creators of DeepSeek are also not very forthcoming in general in publishing the code used to train their model. In the repository for DeepSeek-V3-Base they provide code for loading and quantizing the model, however not much else. This leaves the development process quite obscured, and makes it difficult to replicate the training process of the model.

### Code / architecture documentation
Given the limited extent to which code was published for DeepSeek, the documentation of this code is also very limited. For the most part, code is left to speak for itself. The architecture, however, is fortunately documented a lot better. The architecture is described in detail in the technical report and code modeling it is published on GitHub. With the documentation available, it is at least possible to start building off the model relatively straightforwardly.

### Preprint / paper
Although preprints for every DeepSeek model published thusfar, as of yet none of these papers have been peer-reviewed. It seems that the policy of DeepSeek is to publish their papers entirely independent of the academic community. Given the AI landscape this is an entirely reasonable choice, however it is worth noting that this does distance the models somewhat from academic research development.

### Model card
Though the model card of DeepSeek is quite descriptive, it lacks a few aspects one would normally expect to see in a HuggingFace model card. In particular, it lacks a risk assessment and a discussion of the limitations of the model. This means that although the model card is useful for gathering initial information about the model, it is somewhat less useful for the purposes of continued in-depth research.

### Data sheet
Given that no source data has been published, no data sheet has been released either.

### Package / API
DeepSeek-R1 integrates well with many widely-used packages. As such, although it lacks a package of its own, it is still quite usable in a programming setting. An API is also available, and an app has been published which allows for interfacing with the model with a very low barrier of entry.

### Licensing
The DeepSeek-R1 model makes use of its own custom license which is not a traditional open-source license. From a surface-level read on the part of the index authors it seems that the license is not overly restrictive. However, greater scrutiny is by legal experts is yet preferred.

## Conclusion
In general, it seems that the DeepSeek-R1 model is primarily oriented towards further using it in open-source settings. In this way it is quite similar to Llama, whose development is obscured in a similar way. Although the model claims to be open, its method of open-sourcing for the most part reflect more of a desire to be integrated into the open-source community than to aid it in further research. Although the advances of DeepSeek are commendable, further transparency into the development process would allow it to truly help propel the open-source community forward.