--- 
title: "BERT: The original sin of open-washing large language models"
description: Already in 2018 Google actively marketed the pioneering LLM "BERT" as open source, leading an entire field down a dubious path.
date: 19-12-2024
---

# BERT: The original sin of open-washing large language models


In late 2018, before large language models (LLMs) became mainstream, Google released a large language model bound to shake up the natural language processing (NLP) research community. 
The model dramatically improved the state-of-the-art for many benchmarks in the field, trailblazing the upcoming explosion of interest in LLMs. 
The community was awe-struck by what this model could do and celebrated the new "transformer" machine learning architecture it was build on.
The model even spawned a new scientific field dedicated to the study of it, [BERTology](https://aclanthology.org/2020.tacl-1.54/). 
Amidst the buzz, few thought about asking what data BERT was trained on or whether is was really "open source" as [Google claimed on their website](https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/).

To date, the scientific publication that introduced BERT gained widespread attention in the field (over 125k citations, see Google Scholar). 
Similarly, in subsequent years, work related to the model focussed on its uses, capabilities, and enhancement - rarely asking (critical) questions about how it was built in the first place. 
Dazzled by this technological marvel, xx asked "What We Know About How BERT Works", examining what BERT knows - without mentioning or asking about training data.

We still don't know exactly how the model was built or what data went into it.


this milestone in the history of the field of NLP is also a testament of a longstanding lack of attention to training data over model architecture

Book corpus dataset 

"When talking about Open Source AI, the same set of issues arose over and over again, when discussing training data: What about Data Protection and Privacy? What do we do about different artistic works having different licences depending on the country? What happens if an Open Source AI model discovers one of the images in its training data is copyrighted and cannot be redistributed? How can we ensure legal certainty when all it would take was for a single copyrighted image to be accidentally included in a dataset, to render all AI models using that dataset proprietary (with massive consequences for what they then have to do to comply with the EUâ€™s AI act)?

These are real and genuine concerns, which the FSF has also identified in its work on Open Source AI. SFC has also identified these issues, saying that it may take a decade to come up with a definition."

Open-washing in as old as LLMs are.

::the-index
---
hideFilters: true
filters: 
  view: grid
  models: BERT
---
::

Refs:
https://en.wikipedia.org/wiki/BERT_(language_model)
https://en.wikipedia.org/wiki/BookCorpus
https://research.google/blog/open-sourcing-bert-state-of-the-art-pre-training-for-natural-language-processing/
https://huggingface.co/datasets/defunct-datasets/bookcorpusopen

Papers:
https://aclanthology.org/N19-1423.pdf
https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00349/96482/A-Primer-in-BERTology-What-We-Know-About-How-BERT
